{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value would be the class whose objects would be the nodes of our imaginary DAG for forward pass and back prop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "\n",
    "    def __init__(self,data,_children=(),_op='',label=''):#using tuple because tuple is immutable.\n",
    "        self.data=data\n",
    "        self.grad=0.0 #derivative of final variable wrt this variable\n",
    "        #ofc not applicable for leaf nodes which correspond to input.\n",
    "        #just find prod of the adjacent node if its a * label, else it will be 1.\n",
    "        self._prev=set(_children)\n",
    "        self._op=_op\n",
    "        self.label=label\n",
    "        self._backward=lambda:None # this is the backward function which will be determined at runtime for each Value object. \n",
    "        #by the way we implement it, it would be only for derived nodes and not leaf nodes.\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data:{self.data})\"\n",
    "    \n",
    "    def __add__(self,other):\n",
    "        other = other if isinstance(other,Value) else Value(other) # so that (value object) + (abstract datatype) makes sense.\n",
    "        out=Value(self.data+other.data,(self,other),'+')\n",
    "        def _backward(): # this fills in the derivatives of children\n",
    "            self.grad+=1.0*out.grad #using += in case same node is used more than once, eg: b=a+a or (d=a+b and c=a*b) and we don't override the definition and instead\n",
    "            #sum up the derivatives of all the instances of the node\n",
    "            other.grad+=out.grad #adding in\n",
    "        out._backward=_backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self,other): # fallback function for 2+(value object)\n",
    "        return self+other\n",
    "\n",
    "    def __mul__(self,other):\n",
    "        other = other if isinstance(other,Value) else Value(other)\n",
    "        out=Value(self.data*other.data,(self,other),'*')\n",
    "        def _backward():\n",
    "            self.grad+=other.data*out.grad\n",
    "            other.grad+=self.data*out.grad\n",
    "        out._backward=_backward\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self,other): # so that (abstract data type)*self makes sense\n",
    "        return self*other\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self*-1\n",
    "    \n",
    "    def __sub__(self,other):\n",
    "        return self+(-other)\n",
    "    \n",
    "    def __rsub__(self,other):\n",
    "        return self+(other)\n",
    "    \n",
    "    def __pow__(self,other):\n",
    "        assert isinstance(other,(int,float))\n",
    "        out = Value(pow(self.data,other),(self,),f\"**{other}\")\n",
    "        def _backward():\n",
    "            self.grad+=out.grad*other*(self.data**(other-1))\n",
    "        out._backward=_backward\n",
    "        return out\n",
    "    \n",
    "    def __truediv__(self,other):\n",
    "        return self*(other**-1)\n",
    "    \n",
    "    def __rtruediv__(self,other): #other/self\n",
    "        return other * (self**-1)\n",
    "    #needed to be defined even though in this case self*(other**-1) will go to rmul, but truediv will only be called in case of self/other\n",
    "    # and not other/self.\n",
    "\n",
    "    def tanh(self):\n",
    "        x=self.data\n",
    "        res = (math.exp(2*x)-1)/(math.exp(2*x)+1)\n",
    "        out =Value(res,(self,),_op='tanh')\n",
    "        def _backward():\n",
    "            self.grad+=(1-(out.data**2))*out.grad\n",
    "        out._backward=_backward\n",
    "        return out\n",
    "\n",
    "    def exp(self):\n",
    "        out = Value(math.exp(self.data),(self,),'exp')\n",
    "        def _backward():\n",
    "            self.grad+=out.data*out.grad\n",
    "        out._backward=_backward\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        out = Value(0 if self.data<=0 else self.data,(self,),'relu')\n",
    "        def _backward():\n",
    "            self.grad+=out.grad*(0 if out.data==0 else 1)\n",
    "        out._backward=_backward\n",
    "        return out\n",
    "\n",
    "    def sigmoid(self):\n",
    "        res = 1/(1+math.exp(-self.data))\n",
    "        out = Value(res,(self,),'sigmoid')\n",
    "        def _backward():\n",
    "            self.grad+=out.grad*(out.data*(1-out.data))\n",
    "        out._backward=_backward\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class Neuron:\n",
    "    def __init__(self,n_in):\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(n_in)]\n",
    "        self.b = Value(random.uniform(-1,1))\n",
    "    def __call__(self,x): # call directly on the object.\n",
    "        z = sum((wi*xi for wi,xi in zip(self.w,x)) ,self.b)\n",
    "        act = z.tanh()\n",
    "        return act\n",
    "    def parameters(self):\n",
    "        return self.w+[self.b]\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self,nin,nout): # nin means input dimensions and nout means output dimensions or no of units\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "    def __call__(self,x):\n",
    "        outs=[n(x) for n in self.neurons ]\n",
    "        return outs[0] if len(outs)==1 else outs\n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "class MLP:\n",
    "    def __init__(self,nin,nouts):\n",
    "        self.layers=[]\n",
    "        for out in nouts:\n",
    "            self.layers.append(Layer(nin,out))\n",
    "            nin=out\n",
    "    def __call__(self,x):\n",
    "        for layer in self.layers:\n",
    "            x=layer(x)\n",
    "        return x\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(3,[4,4,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.994820397534473\n",
      "1 5.6422608157698795\n",
      "2 5.397586529702362\n",
      "3 5.218915550245745\n",
      "4 5.063944903977825\n",
      "5 4.892966008489986\n",
      "6 4.655943320248902\n",
      "7 4.260485171391139\n",
      "8 3.5664810048888365\n",
      "9 2.7600172681386166\n",
      "10 2.336741929958314\n",
      "11 2.099976712737065\n",
      "12 1.9194364149351675\n",
      "13 1.7673984878504367\n",
      "14 1.6329625196546438\n",
      "15 1.510130045850243\n",
      "16 1.3952551949545005\n",
      "17 1.2860926223810505\n",
      "18 1.1813304131171396\n",
      "19 1.0803484187829493\n"
     ]
    }
   ],
   "source": [
    "for k in range(20):\n",
    "  \n",
    "  # forward pass\n",
    "  ypred = [mlp(x) for x in xs]\n",
    "  loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n",
    "  \n",
    "  # backward pass\n",
    "  for p in mlp.parameters():\n",
    "    p.grad = 0.0\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  for p in mlp.parameters():\n",
    "    p.data += -0.01 * p.grad\n",
    "  \n",
    "  print(k, loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def test_sanity_check():\n",
    "\n",
    "    x = Value(-4.0)\n",
    "    z = 2 * x + 2 + x\n",
    "    q = z.relu() + z * x\n",
    "    h = (z * z).relu()\n",
    "    y = h + q + q * x\n",
    "    y.backward()\n",
    "    xmg, ymg = x, y\n",
    "\n",
    "    x = torch.Tensor([-4.0]).double()\n",
    "    x.requires_grad = True\n",
    "    z = 2 * x + 2 + x\n",
    "    q = z.relu() + z * x\n",
    "    h = (z * z).relu()\n",
    "    y = h + q + q * x\n",
    "    y.backward()\n",
    "    xpt, ypt = x, y\n",
    "\n",
    "    # forward pass went well\n",
    "    assert ymg.data == ypt.data.item()\n",
    "    # backward pass went well\n",
    "    assert xmg.grad == xpt.grad.item()\n",
    "\n",
    "def test_more_ops():\n",
    "\n",
    "    a = Value(-4.0)\n",
    "    b = Value(2.0)\n",
    "    c = a + b\n",
    "    d = a * b + b**3\n",
    "    c += c + 1\n",
    "    c += 1 + c + (-a)\n",
    "    d += d * 2 + (b + a).relu()\n",
    "    d += 3 * d + (b - a).relu()\n",
    "    e = c - d\n",
    "    f = e**2\n",
    "    g = f / 2.0\n",
    "    g += 10.0 / f\n",
    "    g.backward()\n",
    "    amg, bmg, gmg = a, b, g\n",
    "\n",
    "    a = torch.Tensor([-4.0]).double()\n",
    "    b = torch.Tensor([2.0]).double()\n",
    "    a.requires_grad = True\n",
    "    b.requires_grad = True\n",
    "    c = a + b\n",
    "    d = a * b + b**3\n",
    "    c = c + c + 1\n",
    "    c = c + 1 + c + (-a)\n",
    "    d = d + d * 2 + (b + a).relu()\n",
    "    d = d + 3 * d + (b - a).relu()\n",
    "    e = c - d\n",
    "    f = e**2\n",
    "    g = f / 2.0\n",
    "    g = g + 10.0 / f\n",
    "    g.backward()\n",
    "    apt, bpt, gpt = a, b, g\n",
    "\n",
    "    tol = 1e-6\n",
    "    # forward pass went well\n",
    "    assert abs(gmg.data - gpt.data.item()) < tol \n",
    "    # backward pass went well\n",
    "    assert abs(amg.grad - apt.grad.item()) < tol\n",
    "    assert abs(bmg.grad - bpt.grad.item()) < tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sanity_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.70408163265306   24.70408163265306\n"
     ]
    }
   ],
   "source": [
    "test_more_ops()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A\n",
    "- Will the DAG be valid if the data of a node changes? nope but it don't matter because once we update the weights, we will do a forward pass again in which new nodes/DAG would be created.\n",
    "- How much to swing the parameter to reduce the loss (see the loss vs parameter parabola), is determined by the optimizer algo chosen + learning rate\n",
    "- L2 Regularization prevents the model parameters from getting extreme absolute values. If a parameter has too large a value, its grad will be large too and the param's value will be decreased. If a parameter has a too large negative value, its derivative would be large negative too, which will result in increase of parameter value. This prevents overfitting of model to the training data because overfitting happens when the parameters of a model gain extreme values because of biased training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
